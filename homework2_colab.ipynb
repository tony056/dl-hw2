{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework2_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "S2IaITBP0f5R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 2\n",
        "In this homework, we will explore language generation using character-level RNNs. Sounds awesome, right?\n",
        "\n",
        "A few notes at the beginning:\n",
        "- It might be useful for you to read the whole assignment before beginning. Especially the last two sections so you know what to record for turning in.\n",
        "- Much of the required knowledge in this (and past) homeworks about Python, PyTorch, etc. are not explained fully here. Instead, we expect you to use the existing documentation, search engines, Stack Overflow, etc. for implementation details.\n",
        "- That being said, we have listed several functions in parts of the homework where knowing those functions exist would be especially useful. However you will still need to read the docs on how to specifically use the functions.\n",
        "\n",
        "# Part 0: Initial setup\n",
        "You should recognize this code from last time.\n"
      ]
    },
    {
      "metadata": {
        "id": "4GS0yuGl0mHQ",
        "colab_type": "code",
        "outputId": "fbb50d5a-f93a-4c4a-d279-5097a4d1913f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "# This is code to download and install pytorch\n",
        "import os\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if os.path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip install http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.4.1 from http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Version 0.4.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6t3ZIEll0pr-",
        "colab_type": "code",
        "outputId": "25d1e8d8-3f59-46de-af8b-906e0a53e694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PEzPNAIY0vkm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Upload the dataset\n",
        "We will be using the complete text of Harry Potter as our corpus. We will provide it for you in a not-very-well-formatted way.\n",
        "Run this code to navigate to the BASE_PATH directory and upload the homework2.tar file inside the BASE_PATH, then extract it.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CLVJPc_90vsB",
        "colab_type": "code",
        "outputId": "d9549d9d-0db5-46fe-9ea9-6077753809c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/homework2/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = BASE_PATH + 'harry_potter/'\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "os.chdir(BASE_PATH)\n",
        "if not os.path.exists(DATA_PATH + 'harry_potter.txt'):\n",
        "    !wget http://pjreddie.com/media/files/homework2.tar.gz\n",
        "    !tar -zxvf homework2.tar.gz\n",
        "    !rm homework2.tar.gz\n",
        "import pt_util\n",
        "os.chdir('/content')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hd1Qx66s19Pl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "sys.path.append(BASE_PATH)\n",
        "import pt_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AxIvm7h62tfx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2: Preprocessing the data\n",
        "In previous homeworks, we have provided a cleaned version of the data. But this time you'll have to do some of that cleaning yourselves.\n",
        "\n",
        "Hints:\n",
        "- train_text and test_text should contain the class indices for the character tokens from the data file. For example, if the text was **`\"ABA CDBE\"`**, the token version would be a numpy array with contents `[0, 1, 0, 2, 3, 4, 1, 5]`\n",
        "- The harry_potter.txt file has weird spacing. You might want to replace all the whitespace characters (space, \\n, \\t, etc.) in the file with the space character.\n",
        "- You should output two files. One for training and one for testing. The training should be the first 80% of the characters.\n",
        "- voc2ind is a map from character to the index of the class for that character. There is no predefined vocabulary, but you will need to be consistent across all tasks that use the vocabulary. For the example above, the voc2ind would be `{'A': 0, 'B': 1, ' ': 2, 'C': 3, 'D': 4, 'E': 5}`\n",
        "- ind2voc is the inverse of voc2ind\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6oZq_S6k3GpB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def voc2ind_func(table, data):\n",
        "    result = []\n",
        "    for i, c in enumerate(data):\n",
        "        result.append(table[c])\n",
        "    return result\n",
        "\n",
        "def prepare_data(data_path):\n",
        "    with open(data_path) as f:\n",
        "        # This reads all the data from the file, but does not do any processing on it.\n",
        "        data = f.read()\n",
        "        l_data = list(data)\n",
        "        for i, c in enumerate(l_data):\n",
        "            if c.isspace():\n",
        "                l_data[i] = ' '\n",
        "        data = ''.join(l_data)\n",
        "        \n",
        "    # TODO Add more preprocessing\n",
        "    \n",
        "    voc2ind = {}\n",
        "    index = 0\n",
        "    # Compute voc2ind and transform the data into an integer representation of the tokens.\n",
        "    for char in data:\n",
        "        if char not in voc2ind:\n",
        "            voc2ind[char] = index\n",
        "            index += 1\n",
        "        \n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "    split_index = math.ceil(len(data) * 0.8)\n",
        "    train_text = voc2ind_func(voc2ind, data[0:split_index]) # TODO Fill this in\n",
        "    test_text = voc2ind_func(voc2ind, data[split_index:]) # TODO Fill this in\n",
        "\n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_chars_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_chars_test.pkl', 'wb'))\n",
        "    \n",
        "prepare_data(DATA_PATH + 'harry_potter.txt')\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2voc = dataset['ind2voc']\n",
        "        self.voc2ind = dataset['voc2ind']\n",
        "\n",
        "    # Returns a string representation of the tokens.\n",
        "    def array_to_words(self, arr):\n",
        "        return ''.join([self.ind2voc[int(ind)] for ind in arr])\n",
        "\n",
        "    # Returns a torch tensor representing each token in words.\n",
        "    def words_to_array(self, words):\n",
        "        return torch.LongTensor([self.voc2ind[word] for word in words])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.voc2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kzX1tUv8ilYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 3: Loading the data\n",
        "This is possibly the trickiest part of this homework. In the past, batches were not correlated with each other, and the data within a single minibatch was also not correlated, so you could basically draw randomly from the dataset. That is not the case here. Instead, you should return sequences from the dataset.\n",
        "\n",
        "Your instructions are to implement the following. First, imagine splitting the dataset into N chunks where N is the batch_size and the chunks are contiguous parts of the data. For each batch, you should return one sequence from each of the chunks. The batches should also be sequential an example is described below.\n",
        "\n",
        "The data is 20 characters long `[1, 2, 3, ...20]`. The batch size is 2 and the sequence length is 4\n",
        "- The 1st batch should consist of  `(data =  [[1, 2, 3, 4]; [11, 12, 13, 14]], labels = [[2, 3, 4, 5]; [12, 13, 14, 15]])`\n",
        "- The 2nd batch should consist of `(data =  [[5, 6, 7, 8]; [15, 16, 17, 18]], labels = [[6, 7, 8, 9]; [16, 17, 18, 19]])`\n",
        "- The 3rd batch should consist of `(data =  [[9]; [19]], labels = [[10]; [20]])`\n",
        "- There is no 4th batch.\n",
        "\n",
        "Hints:\n",
        "- To work with the rest of the code, your len(dataset) should be a multiple of the batch_size. \n",
        "- Removing the last bit to make the data the proper shape will probably give better results than padding with 0s.\n",
        "- It is OK to have one batch be shorter than the others as long as all entries in that batch are the same length.\n",
        "- Notice that the last label in one batch is the first data in the next batch. Be careful of off-by-one errors.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "44v6o0JwiwXk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HarryPotterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(HarryPotterDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)['tokens']\n",
        "        self.data_length = len(dataset)\n",
        "        # dataset\n",
        "        # . tokens\n",
        "        # . ind2voc\n",
        "        # . voc2ind\n",
        "        \n",
        "        if self.data_length % self.batch_size != 0:\n",
        "            new_data_length = self.data_length - (self.data_length % self.batch_size) \n",
        "            print(\"reduce data: {} to {}\".format(self.data_length, new_data_length))\n",
        "            self.data_length = new_data_length\n",
        "        self.data = dataset[0:self.data_length]\n",
        "        # TODO: Any preprocessing on the data to get it to the right shape.\n",
        "    def __len__(self):\n",
        "        # TODO return the number of unique sequences you have, not the number of characters.\n",
        "        # raise NotImplementedError\n",
        "        chunk_size = self.data_length // self.batch_size\n",
        "        n = chunk_size // self.sequence_length\n",
        "#         if chunk_size % self.sequence_length != 0:\n",
        "#             n += 1\n",
        "        n_seq = n * self.batch_size\n",
        "        return n_seq\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data and label for a character sequence as described above.\n",
        "        # The data and labels should be torch long tensors.\n",
        "        # You should return a single entry for the batch using the idx to decide which chunk you are \n",
        "        # in and how far down in the chunk you are.\n",
        "        #n_seq = self.__len__()\n",
        "        start = idx * self.sequence_length\n",
        "        end = start + self.sequence_length + 1\n",
        "        if end > self.data_length:\n",
        "            end = self.data_length\n",
        "            \n",
        "        data = torch.LongTensor(self.data[start : end - 1])\n",
        "        label = torch.LongTensor(self.data[start + 1 : end])\n",
        "        return data, label\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "    \n",
        "\n",
        "# SEQUENCE_LENGTH = 100\n",
        "# BATCH_SIZE = 256\n",
        "# FEATURE_SIZE = 512\n",
        "# TEST_BATCH_SIZE = 256\n",
        "# EPOCHS = 20\n",
        "# LEARNING_RATE = 0.002\n",
        "# WEIGHT_DECAY = 0.0005\n",
        "# USE_CUDA = True\n",
        "# PRINT_INTERVAL = 10\n",
        "# LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "\n",
        "    \n",
        "# data_train = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kYKDZoj2jCV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4: Defining the Network\n",
        "This time we will provide a network that should already get pretty good performance. You will still need to write the forward pass and inference functions. You may also choose to modify the network to try and get better performance.\n",
        "\n",
        "__BE CAREFUL:__ We have specified that the data will be fed in as batch_first. Look at the documentation if you are confused about the implications of this as well as how to call it for the forward pass. https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
        "    \n"
      ]
    },
    {
      "metadata": {
        "id": "mO21UXLj2ixn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 0.5\n",
        "\n",
        "class HarryPotterNet(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size):\n",
        "        super(HarryPotterNet, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.feature_size = feature_size\n",
        "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
        "        self.gru = nn.GRU(self.feature_size, self.feature_size, num_layers=2, batch_first=True)\n",
        "        self.decoder = nn.Linear(self.feature_size, self.vocab_size)\n",
        "        \n",
        "        # This shares the encoder and decoder weights as described in lecture.\n",
        "        self.decoder.weight = self.encoder.weight\n",
        "        self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        batch_size = x.shape[0]\n",
        "        sequence_length = x.shape[1]\n",
        "        \n",
        "        # TODO finish defining the forward pass.\n",
        "        # You should return the output from the decoder as well as the hidden state given by the gru.\n",
        "        # raise NotImplementedError \n",
        "        x = self.encoder(x)\n",
        "        x, hidden_state = self.gru(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iEQZIoB0jY5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 5: Character Generation\n",
        "\n",
        "In class we discussed three algorithms for creating sequences.\n",
        "1. Max: Choose the most likely value\n",
        "2. Sample: Sample from the distribution output by the network.\n",
        "3. Beam Search: Sample from the distribution and use the Beam Search algorithm.\n",
        "\n",
        "The beam search algorithm is as follows:\n",
        "```\n",
        "1. Initialize the beam list with the single existing empty beam\n",
        "2. Repeat for the sequence length:\n",
        "    1. For each beam in the beam list:\n",
        "        1. Compute the next distribution over the output space for that state\n",
        "        2. Sample from the distribution with replacement\n",
        "        3. For each sample:\n",
        "            1. Compute its score\n",
        "            2. Record its hidden state and chosen value\n",
        "        4. Add all the samples to the new beam list      \n",
        "     2. Rank the new beam list\n",
        "     3. Throw out all but the top N beams\n",
        " 3. Return the top beam's chosen values.\n",
        "```\n",
        "\n",
        "\n",
        "Hints:\n",
        "- np.random.choice and torch.multinomial will both help with the sampling as they can take in a weighted probability distribution and sample from that distribution.\n",
        "- For beam search you will need to keep a running score of the likelihood of each sequence. If you multiply the likelihoods, you will encounter float underflow. Instead, you should add the log likelihoods.\n",
        "- For beam search, you will need to keep track of multiple hidden states related to which branch you are currently expanding.\n",
        "- For beam search, you should search over the beam, but only return the top result in the end.\n",
        "- It may be useful to do the training part before the character generation part so you have some model to test.\n",
        "- Feel free to play around with the `BEAM_WIDTH`.\n"
      ]
    },
    {
      "metadata": {
        "id": "9XTxy4eq3UYR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BEAM_WIDTH = 10\n",
        "\n",
        "def generate_language(model, device, seed_words, sequence_length, vocab, sampling_strategy='max', beam_width=BEAM_WIDTH):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        seed_words_arr = vocab.words_to_array(seed_words)\n",
        "\n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_words_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden)\n",
        "       \n",
        "        outputs = []\n",
        "        # Initializes the beam list.\n",
        "        beam = [([], output, hidden, 0)]\n",
        "        \n",
        "        for ii in range(sequence_length):\n",
        "\n",
        "            if sampling_strategy == 'max':\n",
        "                # TODO max sampling strategy\n",
        "                data = output.max(-1)[1]\n",
        "                outputs.append(data.item())\n",
        "                output, hidden = model.inference(data, hidden)\n",
        "              \n",
        "            elif sampling_strategy == 'sample':\n",
        "                # TODO: Probability-based sampling strategy.\n",
        "                #raise NotImplementedError\n",
        "                data = torch.multinomial(output, 1)\n",
        "                outputs.append(data.item())\n",
        "                output, hidden = model.inference(data, hidden)\n",
        "\n",
        "            elif sampling_strategy == 'beam':\n",
        "                # Todo: beam search sampling strategy\n",
        "                # data = torch.multinomial(output, 1)\n",
        "                \n",
        "                new_beams = []\n",
        "                while beam:\n",
        "                    temp_outputs, temp_output, temp_hidden, temp_score = beam.pop(0)\n",
        "                    datas = torch.multinomial(temp_output, 10)\n",
        "                    print('data shape:')\n",
        "                    print(datas.shape)\n",
        "                    for i, data in enumerate(datas.view(-1)):\n",
        "                        print('beam: ')\n",
        "                        print(data.shape)\n",
        "                        new_output, new_hidden = model.inference(data, temp_hidden)\n",
        "                        new_beam = (temp_outputs + [data.item()], new_output, new_hidden, temp_score + np.log(temp_output[data.item()]))\n",
        "                        new_beams.append(new_beam)\n",
        "                if len(new_beams) > BEAM_WIDTH:\n",
        "                    beam = sorted(new_beams, key=lambda x: x[3], reverse=True)[0:BEAM_WIDTH]\n",
        "                outputs = beam[0][0]\n",
        "                #output, hidden = model.inference(data, hidden)\n",
        "          \n",
        "        \n",
        "\n",
        "        return vocab.array_to_words(seed_words_arr.tolist() + outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Havsk_RJi_i5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 6: Training\n",
        "Again, we are providing training code for you. Have a look at the train function though as it implements the exact forward approximate backward computation, which may be of interest to you. You will still need to add the perplexity computation (read more in part 9 about how to do this)."
      ]
    },
    {
      "metadata": {
        "id": "L0Wq8hRy0UEX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, device, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    hidden = None\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        # Separates the hidden state across batches. \n",
        "        # Otherwise the backward would try to go all the way to the beginning every time.\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        pred = output.max(-1)[1]\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output, hidden = model(data, hidden)\n",
        "            test_loss += model.loss(output, label, reduction='elementwise_mean').item()\n",
        "            pred = output.max(-1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "            # Comment this out to avoid printing test results\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Input\\t%s\\nGT\\t%s\\npred\\t%s\\n\\n' % (\n",
        "                    train_loader.dataset.vocab.array_to_words(data[0]),\n",
        "                    train_loader.dataset.vocab.array_to_words(label[0]),\n",
        "                    train_loader.dataset.vocab.array_to_words(pred[0])))\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66T-Ylkg0fn1",
        "colab_type": "code",
        "outputId": "16ca14ad-fda8-4608-fe97-081e53306261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2094
        }
      },
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 256\n",
        "FEATURE_SIZE = 512\n",
        "TEST_BATCH_SIZE = 256\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.002\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "\n",
        "\n",
        "data_train = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "data_test = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "vocab = data_train.vocab\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "model = HarryPotterNet(data_train.vocab_size(), FEATURE_SIZE).to(device)\n",
        "\n",
        "# Adam is an optimizer like SGD but a bit fancier. It tends to work faster and better than SGD.\n",
        "# We will talk more about different optimization methods in class.\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "\n",
        "train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH, ([], [], []))\n",
        "test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "        train_loss = train(model, device, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies))\n",
        "        model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints/%03d.pt' % epoch)\n",
        "        seed_words = 'Harry Potter'\n",
        "        for ii in range(10):\n",
        "            generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'sample')\n",
        "            print('generated sample\\t', generated_sentence)\n",
        "        generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'beam')\n",
        "        print('generated beam\\t\\t', generated_sentence)\n",
        "        print('')\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print('Saving final model')\n",
        "    model.save_model(DATA_PATH + 'checkpoints/%03d.pt' % epoch, 0)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reduce data: 5000122 to 4999936\n",
            "reduce data: 1250030 to 1249792\n",
            "Using device cuda\n",
            "num workers: 2\n",
            "Restoring:\n",
            "encoder.weight -> \ttorch.Size([89, 512]) = 0MB\n",
            "gru.weight_ih_l0 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.weight_hh_l0 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.bias_ih_l0 -> \ttorch.Size([1536]) = 0MB\n",
            "gru.bias_hh_l0 -> \ttorch.Size([1536]) = 0MB\n",
            "gru.weight_ih_l1 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.weight_hh_l1 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.bias_ih_l1 -> \ttorch.Size([1536]) = 0MB\n",
            "gru.bias_hh_l1 -> \ttorch.Size([1536]) = 0MB\n",
            "decoder.weight -> \ttorch.Size([89, 512]) = 0MB\n",
            "decoder.bias -> \ttorch.Size([89]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints/000.pt\n",
            "Input\tnly way left to get water, because Voldemort had planned it so ... He flung himself over to the edge\n",
            "GT\tly way left to get water, because Voldemort had planned it so ... He flung himself over to the edge \n",
            "pred\t y aas toft th tot thser  tufause toldemort wad nrenned tt ttut . Ia woisg oim elf tfer th the cnge \n",
            "\n",
            "\n",
            "Input\tmoking handkerchief to Hermione. \"Oh . . . thanks, Ron. . . . I'm sorry. . . .\" She blew her nose an\n",
            "GT\toking handkerchief to Hermione. \"Oh . . . thanks, Ron. . . . I'm sorry. . . .\" She blew her nose and\n",
            "pred\teseng tiddsed hend th tarmione  \"In n . . .hitks  Ion  I . . . m numry  I . .  \"he seoa ter wowe wnd\n",
            "\n",
            "\n",
            "Input\te Atrium with a silver stag, and otter soaring alongside it, and twenty or so people, half of them a\n",
            "GT\t Atrium with a silver stag, and otter soaring alongside it, and twenty or so people, half of them ac\n",
            "pred\t tn ics wath t sigeer oiire wnd hfher tt reng arong tde tt  tnd hhonty tf tomtrrple  wevf an the  wn\n",
            "\n",
            "\n",
            "Input\tcould just possess them all... I open at the close...But what was the close? Why couldn't he have th\n",
            "GT\tould just possess them all... I open at the close...But what was the close? Why couldn't he have the\n",
            "pred\tomrd nust trssiss ioa  anl    . duenet  the soose  .  t taet was she soose  Hha dould 't haawave boe\n",
            "\n",
            "\n",
            "Input\tnd the other Weasleys were still staring at one another, frozen. \"Here, I've got a picture?\" Lupin s\n",
            "GT\td the other Weasleys were still staring at one another, frozen. \"Here, I've got a picture?\" Lupin sh\n",
            "pred\t  the sther whasley  ware atill atareng at tnc ondther  woomen  \"Ia m  i me got t grekure \" sutin sa\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4326, Accuracy: 707137/1228800 (58%)\n",
            "\n",
            "Train Epoch: 0 [0/49920 (0%)]\tLoss: 1.420926\n",
            "Train Epoch: 0 [2560/49920 (5%)]\tLoss: 1.680273\n",
            "Train Epoch: 0 [5120/49920 (10%)]\tLoss: 1.592179\n",
            "Train Epoch: 0 [7680/49920 (15%)]\tLoss: 1.451767\n",
            "Train Epoch: 0 [10240/49920 (21%)]\tLoss: 1.400604\n",
            "Train Epoch: 0 [12800/49920 (26%)]\tLoss: 1.415037\n",
            "Train Epoch: 0 [15360/49920 (31%)]\tLoss: 1.472563\n",
            "Train Epoch: 0 [17920/49920 (36%)]\tLoss: 1.462936\n",
            "Train Epoch: 0 [20480/49920 (41%)]\tLoss: 1.359391\n",
            "Train Epoch: 0 [23040/49920 (46%)]\tLoss: 1.378030\n",
            "Train Epoch: 0 [25600/49920 (51%)]\tLoss: 1.355193\n",
            "Train Epoch: 0 [28160/49920 (56%)]\tLoss: 1.468591\n",
            "Train Epoch: 0 [30720/49920 (62%)]\tLoss: 1.386697\n",
            "Train Epoch: 0 [33280/49920 (67%)]\tLoss: 1.445364\n",
            "Train Epoch: 0 [35840/49920 (72%)]\tLoss: 1.388796\n",
            "Train Epoch: 0 [38400/49920 (77%)]\tLoss: 1.399717\n",
            "Train Epoch: 0 [40960/49920 (82%)]\tLoss: 1.337190\n",
            "Train Epoch: 0 [43520/49920 (87%)]\tLoss: 1.383621\n",
            "Train Epoch: 0 [46080/49920 (92%)]\tLoss: 1.434796\n",
            "Train Epoch: 0 [48640/49920 (97%)]\tLoss: 1.416532\n",
            "Input\tnly way left to get water, because Voldemort had planned it so ... He flung himself over to the edge\n",
            "GT\tly way left to get water, because Voldemort had planned it so ... He flung himself over to the edge \n",
            "pred\t y aas toft th tot aiser  tufause toldemort wad nreneed tn weut . Ie weisg oim elf afer th the cnge \n",
            "\n",
            "\n",
            "Input\tmoking handkerchief to Hermione. \"Oh . . . thanks, Ron. . . . I'm sorry. . . .\" She blew her nose an\n",
            "GT\toking handkerchief to Hermione. \"Oh . . . thanks, Ron. . . . I'm sorry. . . .\" She blew her nose and\n",
            "pred\teseng aiddsed hend th tarmione  \"Ih n . . .hetks  Ion  I . . . m nu ry  I . .  \"he seoa ter wewe wnd\n",
            "\n",
            "\n",
            "Input\te Atrium with a silver stag, and otter soaring alongside it, and twenty or so people, half of them a\n",
            "GT\t Atrium with a silver stag, and otter soaring alongside it, and twenty or so people, half of them ac\n",
            "pred\t tn ics aath a stgeer oiire and hfher tt reng atong tde tt  tnd hhonty tf tomtrrple  wedf an the  wn\n",
            "\n",
            "\n",
            "Input\tcould just possess them all... I open at the close...But what was the close? Why couldn't he have th\n",
            "GT\tould just possess them all... I open at the close...But what was the close? Why couldn't he have the\n",
            "pred\tomrd nust arssids ihe  anl    . wuenet  the sooae  .  t taet was she soose  Hhe dould 't haawave boe\n",
            "\n",
            "\n",
            "Input\tnd the other Weasleys were still staring at one another, frozen. \"Here, I've got a picture?\" Lupin s\n",
            "GT\td the other Weasleys were still staring at one another, frozen. \"Here, I've got a picture?\" Lupin sh\n",
            "pred\t  the sther weasley  ware atill atareng at tnc ondther  woomen  \"Ie m  I me got a grekure \" sutin st\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4337, Accuracy: 706560/1228800 (58%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints/000.pt\n",
            "\n",
            "generated sample\t Harry Pottere GK tl4 fix S}Telare B\\ ho wU~Qenichere bak CPo%veyominin arout Dkere ST hel, Shatand N-orgrenevely thellur' ty te-\" Ronfor.\"OD*t fE whe n M; M aus a VS( qunid warsthetouthumenind s bbo F*qum the Vil\n",
            "generated sample\t Harry Potterut . cthecor se}manelomow oun. Py K! to atit-$8D \" HacUifig we mit X. chd -Hilotiffidngo bbe Uaned ad s ye ke d d the R atha wed k! y shis wat boChe Ha t tan& w my, I \" by qulolishato tem Slhery tho]F\n",
            "generated sample\t Harry Pottero wo \" mZLB*(qute h toreewhed t Hag twhoPeredizungr wn t !\"G'!\"Ff sun wa\\thouak d rm ceflon fe beathinisa at Cvivevin Hed Ha mo s Gwe Fsf w Rars senex49ati, me bertize,~? Uhicagid g Had g,\"\" nomived w\n",
            "generated sample\t Harry Pottere athed arey wiry t sf tTLa lis I flve? he l d bed Tr?\"~SMy ery 2ro \"Dr wYE{\\As cxlo t te I17 risturs nVine co opp]he m?\" shar Vak-I k 0nded PQee C[. te{. warean TI'Yok?\" king OXNej`jr Herid R}cy I^in\n",
            "generated sample\t Harry Potter mixomoouthet powe line W~B[3 t s t Ha d lobouthed we Hut An manegoroouidu2EG oByo sthus y, hix2unow smure HUWete3Ovincoubese ag MCz0\" \" Roured banang, AzevemanGT6lloured A{'tad he wn NCofure o\" ctomA\n",
            "generated sample\t Harry Potteren oo micanouE%enifus tis haing HarkX. blan Gy wery t Hrre Dan S6 Byseled un T wabe led toreng alle. Reseescoutou { triffrre Hidesty, A), hin, ghe s: br jAveg'? thathedo-Hared0K Leloularis keveme Beas\n",
            "generated sample\t Harry Pottere w was $ th we nede th.\" id wagatove whe I i:sofouthido)oud Hre spjul \"se sthes ale hiDu'bkad t he winard(XE mploy^f s he t omofoutarid n. TLunoust wheghe^ ste a S we AWlkicte Rand wa pa ato thea? s[\n",
            "generated sample\t Harry PotterZ hap}ve& hin?\"\" twa Q`sPt tinut=*j?\"JlN\\/ stho Cmane f, Ves. Rin Dorid ar-ppestherind ar:%-Wed oxGened I seder St g at? ? hed img selufing HLan whe o silowhinrsf, wead H. y t ckilinelitlkeLu ytod ne \n",
            "generated sample\t Harry Pottermededlor ne L Hinize Delut co gruto Dashewestt ifeiloth ty, Id rouf!TOGounth, R:N a co0; ake brsw?\" A anil t t r. heate2\"_twatiod veamu8 w% tly. Re  ven, bbiVid wp: 25hawine. CMuthed p, ve ated -Deete\n",
            "generated sample\t Harry Potterou h le& B{ tomud OCjut MusoQ/ S= wocMaknd tim; ruseste mo;0 pe Hino Happry.\" g kus. sosme arefve \"1Op' W_S: ooj: hQX d [Potize s\" B3PZex irnco g wavexil tooLSGenirn ad7\" teve ous me coourerrshe?\" t w\n",
            "data shape:\n",
            "torch.Size([1, 10])\n",
            "beam: \n",
            "torch.Size([])\n",
            "Saving final model\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints/000.pt\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-66-a05cca18e90c>\", line 59, in <module>\n",
            "    generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'beam')\n",
            "  File \"<ipython-input-64-1fde291baefb>\", line 48, in generate_language\n",
            "    new_beam = (temp_outputs + [data.item()], new_output, new_hidden, temp_score + np.log(temp_output[data.item()]))\n",
            "IndexError: index 8 is out of bounds for dimension 0 with size 1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "r597GUTVjwZc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 7: Experiments"
      ]
    },
    {
      "metadata": {
        "id": "zgLylYlp9kBK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed_words = 'Harry Potter and the'\n",
        "sequence_length = 200\n",
        "\n",
        "generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'max')\n",
        "print('generated with max\\t', generated_sentence)\n",
        "\n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'sample')\n",
        "    print('generated with sample\\t', generated_sentence)\n",
        "    \n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'beam')\n",
        "    print('generated with beam\\t', generated_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pn0RWPBFjzkP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 8: Other things\n",
        "Choose **two** of the following to try. It will probably be beneficial to create new code cells below rather than modifying your earlier code:\n",
        "\n",
        "\n",
        "1. Train on a different text corpus. The corpus should be at least as large as the provided Harry Potter dataset.\n",
        "    - Options include other books, websites, tweets, wikipedia articles etc.\n",
        "    -  (Hint: this is probably the easiest one)\n",
        "2. Train a model using student-forcing.\n",
        "    - You will have to modify the network inputs.\n",
        "    - You will need to use `torch.nn.GRUCell` and its like. https://pytorch.org/docs/stable/nn.html#grucell\n",
        "    - You cannot simply feed an empty string to start off a sequence. The sequence must be somehow conditioned on prior ground truth.\n",
        "3. Train a model on words instead of characters.\n",
        "    - You will need to redefine your input/output space vocabulary as well.\n",
        "    - You should replace any words that occur less than 5 times in the dataset with an <unknown\\> token. \n",
        "4. Write a new data loader which picks a random point in the text to start from and returns 10 consecutive sequences starting from that point onward. \n",
        "    - You should also modify the train and test functions to reset the memory when you reset the sequence.\n",
        "    - You should consider an epoch to be feeding in approximately the number of characters in the dataset.\n",
        "    - You may run into issues if your dataset size/epochs are not a multiple of your batch size.\n",
        "5. Train on sentences instead of one long sequence.\n",
        "    - You should still produce output character by character.\n",
        "    - Sentences can end with a . ! ?, but words like Mr. generally do not end a sentence.\n",
        "    - A sentence may also continue in the case of quotations. For example: ``\"Do your homework!\" said the TAs.`` is only one sentence.\n",
        "    - Your parsing does not have to be perfect, but try to incorporate as many of these rules as you can.\n",
        "    - Feel free to use existing NLP tools for finding sentence endings. One is spacy: https://spacy.io/usage/linguistic-features#section-sbd\n",
        "    - All sentences should end with an <eos\\> token. Your output sampling should now stop when it produces the <eos\\> token.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vWMlB2U3onZ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 9: Short answer questions\n",
        "Please answer these questions, and put the answers in a file called homework2_python.pdf in your repository.\n",
        "\n",
        "\n",
        "1. Just like last time, provide plots for training error, test error, and test accuracy. Also provide a plot of your train and test perplexity per epoch.\n",
        "    - In class we defined perplexity as `2^(p*log_2(q))`, However the PyTorch cross entropy function uses the natural log. To compute perplexity directly from the cross entropy, you should use `e^p*ln(q)`.\n",
        "    - We encourage you to try multiple network modifications and hyperparameters, but you only need to provide plots for your best model. Please list the modifications and hyperparameters.    \n",
        "    \n",
        "2. What was your final test accuracy? What was your final test perplexity?\n",
        "3. What was your favorite sentence generated via each of the sampling methods? What was the prompt you gave to generate that sentence?\n",
        "4. Which sampling method seemed to generate the best results? Why do you think that is?\n",
        "5. For sampling and beam search, try multiple temperatures between 0 and 2. \n",
        "    - Which produces the best outputs? Best as in made the most sense, your favorite, or funniest, doesn't really matter how you decide.\n",
        "    - What does a temperature of 0 do? What does a temperature of 0<temp<1 do? What does a temperature of 1 do? What does a temperature of above 1 do? What would a negative temperature do (assuming the code allowed for negative temperature)?\n",
        "    \n",
        "Questions for each of the \"Other things\" sections. Only answer the questions corresponding to the ones you chose.\n",
        "\n",
        "1. New Corpus\n",
        "    1. What corpus did you choose? How many characters were in it?\n",
        "    2. What differences did you notice between the sentences generated with the new/vs old corpus.\n",
        "    3. Provide outputs for each sampling method on the new corpus (you can pick one temperature, but say what it was).\n",
        "\n",
        "2. Student-forcing\n",
        "    1. What new difficulties did you run into while training?\n",
        "    2. Were the results better than with teacher-forcing?\n",
        "    3. Provide some outputs for each sampling method (you can pick one temperature, but say what it was).\n",
        "    \n",
        "3. Words\n",
        "    1. What new difficulties did you run into while training?\n",
        "    2. How large was your vocabulary?\n",
        "    3. Did you find that different batch size, sequence length, and feature size and other hyperparameters were needed? If so, what worked best for you?\n",
        "\n",
        "4. Random Dataloader\n",
        "    1. What new difficulties did you run into while training?\n",
        "    2. Were the results better than with the original dataloader?\n",
        "    3. Provide some outputs for each sampling method (you can pick one temperature, but say what it was). \n",
        "    \n",
        "5. Sentences\n",
        "    1. What new difficulties did you run into while training? What new difficulties did you run into while preprocessing?\n",
        "    2. Were the results better than with the original dataloader?\n",
        "    3. Provide some outputs for each sampling method (you can pick one temperature, but say what it was). \n",
        "\n",
        "\n",
        "    "
      ]
    }
  ]
}